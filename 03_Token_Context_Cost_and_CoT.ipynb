{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f9f7c8",
   "metadata": {},
   "source": [
    "# Notebook 3 — Token/Context Maliyeti + CoT (Kalite vs Maliyet)\n",
    "\n",
    "Bu notebook iki şeyi gösterir:\n",
    "1) Zero/One/Few-shot prompt'larının token tüketimi\n",
    "2) CoT (step-by-step) istemenin maliyet/çıktı etkisi\n",
    "\n",
    "⏱️ Önerilen süre: **12–15 dk**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8bf97",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Bu notebook'lar, LLM çağrısı için **LangChain** kullanır ve otomatik sağlayıcı seçer:\n",
    "\n",
    "- `OPENAI_API_KEY` varsa OpenAI üzerinden gider (varsayılan model: `gpt-4o-mini`)\n",
    "- `GOOGLE_API_KEY` varsa Gemini üzerinden gider (varsayılan model: `gemini-1.5-pro`)\n",
    "\n",
    "> Dilerseniz model adını `OPENAI_MODEL` / `GEMINI_MODEL` ile değiştirebilirsiniz.\n",
    "\n",
    "**Not:** Bu notebook dosyaları örnek amaçlıdır. Kendi kurumsal ortamınızda anahtar yönetimini güvenli şekilde yapın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a359e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lokal/Colab için (gerekliyse) kurulum:\n",
    "%pip -q install -U langchain-core langchain-openai langchain-google-genai pydantic tiktoken python-dotenv pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d3492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "from typing import Dict, Any\n",
    "\n",
    "def get_llm():\n",
    "    # Sağlayıcı seçimi: OpenAI -> Gemini\n",
    "    openai_key = os.getenv('OPENAI_API_KEY')\n",
    "    google_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "    if openai_key:\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        model = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')\n",
    "        llm = ChatOpenAI(model=model, temperature=0.1)\n",
    "        return llm, f'OpenAI ({model})'\n",
    "\n",
    "    if google_key:\n",
    "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "        model = os.getenv('GEMINI_MODEL', 'gemini-1.5-pro')\n",
    "        llm = ChatGoogleGenerativeAI(model=model, temperature=0.1, api_key=google_key)\n",
    "        return llm, f'Google Gemini ({model})'\n",
    "\n",
    "    raise RuntimeError(\n",
    "        'API key bulunamadı. Lütfen ortam değişkeni tanımlayın:\\n'\n",
    "        '- OpenAI için: OPENAI_API_KEY\\n'\n",
    "        '- Gemini için: GOOGLE_API_KEY\\n'\n",
    "        'Opsiyonel: OPENAI_MODEL / GEMINI_MODEL'\n",
    "    )\n",
    "\n",
    "llm, provider = get_llm()\n",
    "print('✅ LLM hazır:', provider)\n",
    "\n",
    "def llm_text(prompt: str) -> str:\n",
    "    resp = llm.invoke(prompt)\n",
    "    return getattr(resp, 'content', str(resp)).strip()\n",
    "\n",
    "def strip_fences(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'^```json\\s*', '', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'^```\\s*', '', s)\n",
    "    s = re.sub(r'\\s*```$', '', s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2dca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini veri seti (email/ticket) — demo için\n",
    "EMAILS = [\n",
    "    {'id': 'E1', 'text': 'Kargom hâlâ gelmedi. 7 gündür bekliyorum. Acil çözüm istiyorum!', 'notes': 'Gecikme + yüksek aciliyet'},\n",
    "    {'id': 'E2', 'text': 'Ürün kırık geldi. Değişim yapabilir miyiz?', 'notes': 'Hasarlı ürün'},\n",
    "    {'id': 'E3', 'text': 'İade sürecini nasıl başlatabilirim? Kutuyu attım ama ürün duruyor.', 'notes': 'İade + edge-case (kutusuz)'},\n",
    "    {'id': 'E4', 'text': 'Kartımdan iki kez çekim yapılmış görünüyor. Lütfen hemen kontrol edin.', 'notes': 'Faturalama + yüksek aciliyet'},\n",
    "    {'id': 'E5', 'text': 'Ürününüzün kullanım kılavuzunu paylaşır mısınız?', 'notes': 'Bilgi talebi (low)'},\n",
    "]\n",
    "len(EMAILS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91e1dd",
   "metadata": {},
   "source": [
    "## 1) Token sayımı\n",
    "\n",
    "OpenAI için `tiktoken` daha doğru sayım verir. Diğer sağlayıcılarda tokenizer farklı olabilir.\n",
    "Burada:\n",
    "- `tiktoken` çalışırsa onu kullanır\n",
    "- çalışmazsa yaklaşık tahmin yapar (1 token ≈ 4 karakter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289ac94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_tokens(text: str, model_hint: str = 'gpt-4o-mini') -> int:\n",
    "    try:\n",
    "        import tiktoken\n",
    "        enc = tiktoken.encoding_for_model(model_hint)\n",
    "        return len(enc.encode(text))\n",
    "    except Exception:\n",
    "        return max(1, len(text) // 4)\n",
    "\n",
    "def estimate_cost(tokens_in: int, tokens_out: int, price_in_per_1k: float, price_out_per_1k: float) -> float:\n",
    "    return (tokens_in/1000)*price_in_per_1k + (tokens_out/1000)*price_out_per_1k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178ad9c",
   "metadata": {},
   "source": [
    "## 2) Üç stratejinin prompt token farkı\n",
    "\n",
    "Notebook 1'deki prompt'ların kısa bir kopyası."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f88460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "SCHEMA = (\n",
    "    'Return ONLY valid JSON with exactly these keys:\\n'\n",
    "    '{\\n'\n",
    "    '  \"category\": \"string\",\\n'\n",
    "    '  \"urgency\": \"low|medium|high\",\\n'\n",
    "    '  \"reason\": \"string (max 1 sentence)\"\\n'\n",
    "    '}\\n'\n",
    "    'No extra text, no markdown, JSON only.'\n",
    ")\n",
    "\n",
    "ONE_EXAMPLE = {\n",
    "    'email': 'My order arrived broken. I want a replacement as soon as possible.',\n",
    "    'answer': {'category':'Damaged product','urgency':'medium','reason':'Customer reports a damaged item and requests a replacement.'}\n",
    "}\n",
    "\n",
    "FEW_EXAMPLES = [\n",
    "    ('Where is my package? It was supposed to arrive 5 days ago.', {'category':'Delivery issue','urgency':'high','reason':'Customer reports a significantly delayed delivery.'}),\n",
    "    ('How can I return the product? I changed my mind.', {'category':'Return request','urgency':'low','reason':'Customer asks for return instructions without a critical issue.'}),\n",
    "    ('You charged me twice for the same order. Fix this immediately.', {'category':'Billing issue','urgency':'high','reason':'Customer reports a double charge and requests urgent resolution.'}),\n",
    "]\n",
    "\n",
    "def prompt_zero(email_text: str) -> str:\n",
    "    return ('You are a customer support triage assistant.\\n'\n",
    "            'Classify the email into a category and urgency.\\n\\n' + SCHEMA + '\\n\\n' + 'Email:\\n' + email_text)\n",
    "\n",
    "def prompt_one(email_text: str) -> str:\n",
    "    return ('You are a customer support triage assistant.\\n'\n",
    "            'Use the example to follow the same output format and labeling style.\\n\\n'\n",
    "            + SCHEMA + '\\n\\n'\n",
    "            + 'Example:\\nEmail:\\n' + ONE_EXAMPLE['email'] + '\\n'\n",
    "            + 'Answer:\\n' + json.dumps(ONE_EXAMPLE['answer'], ensure_ascii=False) + '\\n\\n'\n",
    "            + 'Now classify this email:\\n\\nEmail:\\n' + email_text)\n",
    "\n",
    "def prompt_few(email_text: str) -> str:\n",
    "    ex_block = ''\n",
    "    for mail, ans in FEW_EXAMPLES:\n",
    "        ex_block += 'Email:\\n' + mail + '\\nAnswer:\\n' + json.dumps(ans, ensure_ascii=False) + '\\n\\n'\n",
    "    return ('You are a customer support triage assistant.\\n'\n",
    "            'Follow the same pattern as the examples.\\n\\n' + SCHEMA + '\\n\\n'\n",
    "            + 'Examples:\\n' + ex_block\n",
    "            + 'Now classify this email:\\n\\nEmail:\\n' + email_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375d67d",
   "metadata": {},
   "source": [
    "### Token ölçümü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e4e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample = EMAILS[0]['text']\n",
    "model_hint = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')\n",
    "\n",
    "p0, p1, p2 = prompt_zero(sample), prompt_one(sample), prompt_few(sample)\n",
    "t0, t1, t2 = count_tokens(p0, model_hint), count_tokens(p1, model_hint), count_tokens(p2, model_hint)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {'strategy':'zero-shot', 'prompt_tokens': t0},\n",
    "    {'strategy':'one-shot',  'prompt_tokens': t1},\n",
    "    {'strategy':'few-shot',  'prompt_tokens': t2},\n",
    "]).sort_values('prompt_tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd53f2",
   "metadata": {},
   "source": [
    "## 3) Basit maliyet simülasyonu\n",
    "\n",
    "Fiyatlar modele göre değişir. Buraya kendi fiyatlarını gir.\n",
    "Çıktı token'ı JSON kısa olduğundan genelde 60–120 arası olur (tahmin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_in_per_1k  = 0.0   # örn: 0.15\n",
    "price_out_per_1k = 0.0   # örn: 0.60\n",
    "avg_out_tokens   = 90\n",
    "\n",
    "rows = []\n",
    "for name, tok in [('zero-shot', t0), ('one-shot', t1), ('few-shot', t2)]:\n",
    "    rows.append({\n",
    "        'strategy': name,\n",
    "        'prompt_tokens_in': tok,\n",
    "        'avg_tokens_out': avg_out_tokens,\n",
    "        'est_cost_per_call': estimate_cost(tok, avg_out_tokens, price_in_per_1k, price_out_per_1k)\n",
    "    })\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e2601",
   "metadata": {},
   "source": [
    "## 4) Context şişmesi (örnek sayısı arttıkça)\n",
    "\n",
    "Few-shot örnek sayısını artırınca prompt token'ı nasıl büyüyor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_n_examples(email_text: str, n: int) -> str:\n",
    "    exs = (FEW_EXAMPLES * ((n // len(FEW_EXAMPLES)) + 1))[:n]\n",
    "    ex_block = ''\n",
    "    for mail, ans in exs:\n",
    "        ex_block += 'Email:\\n' + mail + '\\nAnswer:\\n' + json.dumps(ans, ensure_ascii=False) + '\\n\\n'\n",
    "    return ('You are a customer support triage assistant.\\n'\n",
    "            'Follow the same pattern as the examples.\\n\\n' + SCHEMA + '\\n\\n'\n",
    "            + 'Examples:\\n' + ex_block\n",
    "            + 'Now classify this email:\\n\\nEmail:\\n' + email_text)\n",
    "\n",
    "for n in [0, 1, 3, 5, 10, 20, 40]:\n",
    "    p = prompt_with_n_examples(sample, n)\n",
    "    print(f'n={n:>2}  prompt_tokens≈{count_tokens(p, model_hint)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d9715",
   "metadata": {},
   "source": [
    "## 5) CoT (Chain-of-Thought): kalite vs maliyet\n",
    "\n",
    "Burada iki yaklaşımı karşılaştırıyoruz:\n",
    "- **Direct**: sadece sonucu üret\n",
    "- **Step-by-step**: adım adım çöz\n",
    "\n",
    "> Üretimde full CoT’yi kullanıcıya göstermek istemeyebilirsiniz. Alternatif: kısa gerekçe + self-check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f00a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATH_TASK = ('A company sold 120 items on Monday, 95 on Tuesday, and 110 on Wednesday. '\n",
    "             'If 15 items were returned, how many net items were sold?')\n",
    "\n",
    "PROMPT_DIRECT = 'Answer with a single line: \"Net sold = <number>\". Question: ' + MATH_TASK\n",
    "PROMPT_COT    = 'Solve step by step, then answer with a single line: \"Net sold = <number>\". Question: ' + MATH_TASK\n",
    "\n",
    "direct = llm_text(PROMPT_DIRECT)\n",
    "cot    = llm_text(PROMPT_COT)\n",
    "\n",
    "print('DIRECT:\\n', direct)\n",
    "print('\\nCOT:\\n', cot)\n",
    "\n",
    "print('\\nToken estimates (prompt only):')\n",
    "print('direct:', count_tokens(PROMPT_DIRECT, model_hint))\n",
    "print('cot   :', count_tokens(PROMPT_COT, model_hint))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f755a0",
   "metadata": {},
   "source": [
    "## 6) Alternatif: brief rationale + self-check\n",
    "\n",
    "CoT yerine, üretimde daha güvenli ve kısa bir format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8371ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_BRIEF = (\n",
    "    'Return:\\n'\n",
    "    '1) Answer: Net sold = <number>\\n'\n",
    "    '2) Rationale: (max 2 sentences)\\n'\n",
    "    '3) Self-check: (max 2 bullets)\\n\\n'\n",
    "    'Question: ' + MATH_TASK\n",
    ")\n",
    "\n",
    "brief = llm_text(PROMPT_BRIEF)\n",
    "print(brief)\n",
    "print('\\nPrompt token estimate:', count_tokens(PROMPT_BRIEF, model_hint))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd84cff2",
   "metadata": {},
   "source": [
    "## 7) Egzersiz (3–5 dk)\n",
    "\n",
    "1) CoT prompt’unu Türkçe yapıp token sayımını kıyaslayın.\n",
    "2) 10–20 örneğe çıkınca context’in nasıl şiştiğini not edin.\n",
    "3) Brief + self-check formatını farklı bir problemde deneyin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
